{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00880817, -0.02784138, -0.03945461,  0.02059954], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # [pos, vel, angle, angular vel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.009365  , -0.22237594, -0.03904263,  0.30057767], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    return int(obs[2] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = []\n",
    "for episode in range(1000):\n",
    "    curr_reward = 0\n",
    "    obs = env.reset()[0]\n",
    "\n",
    "    for step in range(500):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "        curr_reward += reward\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    episode_reward.append(curr_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68.0, 24.0, 42.086, 8.909354858798698)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(episode_reward), np.min(episode_reward), np.mean(episode_reward), np.std(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE Algorithm with Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for number of iteration:\n",
    "#     Loop for number of episodes:\n",
    "#         Loop for number of steps:\n",
    "#             play one step and get reward and grad\n",
    "#             make an array of reward and grad\n",
    "#         store array in a matrix for each episode\n",
    "#     discount and normalize the reward matrix\n",
    "#     compute reward weighted mean for each step for every model training variables\n",
    "#     update model vars by applying gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, model, obs, loss_fn):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_left_prob = model(obs) # binary classification => probability of going left (1 = left and 0 = right)\n",
    "        action = tf.random.uniform(shape=(1,1)) > pred_left_prob # Explore and Exploit => 0 (left) with prob of pred_left_prob and 1 (right) with prob of 1-pred_left_prob\n",
    "        y_target = [[1.]] - tf.cast(action, float) # treat it as y_true\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, pred_left_prob))\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables) # tuple => grad of loss wrt to each model var\n",
    "    obs, reward, term, trunc, info = env.step(int(action)) # perform the action\n",
    "    return obs[None,:], reward, term, trunc, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(steps, env, model, loss_fn):\n",
    "\n",
    "    obs = env.reset()[0][None,:]\n",
    "    rewards = []\n",
    "    grads = []\n",
    "    for step in range(steps):\n",
    "        obs, reward, term, trunc, grad = play_one_step(env, model, obs, loss_fn)\n",
    "        # storing reward and grad for each step in a list\n",
    "        rewards.append(reward)\n",
    "        grads.append(grad)\n",
    "\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    return rewards, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounting(rewards, discount_factor):\n",
    "    discounted_rewards = np.array(rewards)\n",
    "    for i in range(len(rewards)-2, -1, -1):\n",
    "        discounted_rewards[i] += discounted_rewards[i+1]*discount_factor\n",
    "\n",
    "    return discounted_rewards\n",
    "\n",
    "def normalize_and_discount(episodes_rewards, discount_factor):\n",
    "    \n",
    "    discounted_episodes_rewards = [discounting(rewards, discount_factor) for rewards in episodes_rewards]\n",
    "    flat_rewards = np.concatenate(discounted_episodes_rewards)\n",
    "    mean_reward = flat_rewards.mean()\n",
    "    std_reward = flat_rewards.std()\n",
    "\n",
    "    return [(discounted_rewards - mean_reward)/std_reward for discounted_rewards in discounted_episodes_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "n_ep = 10\n",
    "n_steps = 200\n",
    "discount_factor = 0.95\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = Adam(lr)\n",
    "loss_fn = binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0, Episode :0, Reward : 69.0\n",
      "Iter : 0, Episode :9, Reward : 11.0\n",
      "Iter : 1, Episode :0, Reward : 29.0\n",
      "Iter : 1, Episode :9, Reward : 29.0\n",
      "Iter : 2, Episode :0, Reward : 17.0\n",
      "Iter : 2, Episode :9, Reward : 58.0\n",
      "Iter : 3, Episode :0, Reward : 29.0\n",
      "Iter : 3, Episode :9, Reward : 17.0\n",
      "Iter : 4, Episode :0, Reward : 54.0\n",
      "Iter : 4, Episode :9, Reward : 38.0\n",
      "Iter : 5, Episode :0, Reward : 118.0\n",
      "Iter : 5, Episode :9, Reward : 48.0\n",
      "Iter : 6, Episode :0, Reward : 29.0\n",
      "Iter : 6, Episode :9, Reward : 25.0\n",
      "Iter : 7, Episode :0, Reward : 48.0\n",
      "Iter : 7, Episode :9, Reward : 89.0\n",
      "Iter : 8, Episode :0, Reward : 41.0\n",
      "Iter : 8, Episode :9, Reward : 40.0\n",
      "Iter : 9, Episode :0, Reward : 14.0\n",
      "Iter : 9, Episode :9, Reward : 25.0\n",
      "Iter : 10, Episode :0, Reward : 18.0\n",
      "Iter : 10, Episode :9, Reward : 40.0\n",
      "Iter : 11, Episode :0, Reward : 11.0\n",
      "Iter : 11, Episode :9, Reward : 47.0\n",
      "Iter : 12, Episode :0, Reward : 18.0\n",
      "Iter : 12, Episode :9, Reward : 46.0\n",
      "Iter : 13, Episode :0, Reward : 18.0\n",
      "Iter : 13, Episode :9, Reward : 49.0\n",
      "Iter : 14, Episode :0, Reward : 51.0\n",
      "Iter : 14, Episode :9, Reward : 48.0\n",
      "Iter : 15, Episode :0, Reward : 47.0\n",
      "Iter : 15, Episode :9, Reward : 40.0\n",
      "Iter : 16, Episode :0, Reward : 23.0\n",
      "Iter : 16, Episode :9, Reward : 25.0\n",
      "Iter : 17, Episode :0, Reward : 103.0\n",
      "Iter : 17, Episode :9, Reward : 30.0\n",
      "Iter : 18, Episode :0, Reward : 57.0\n",
      "Iter : 18, Episode :9, Reward : 34.0\n",
      "Iter : 19, Episode :0, Reward : 10.0\n",
      "Iter : 19, Episode :9, Reward : 28.0\n",
      "Iter : 20, Episode :0, Reward : 15.0\n",
      "Iter : 20, Episode :9, Reward : 10.0\n",
      "Iter : 21, Episode :0, Reward : 29.0\n",
      "Iter : 21, Episode :9, Reward : 14.0\n",
      "Iter : 22, Episode :0, Reward : 12.0\n",
      "Iter : 22, Episode :9, Reward : 31.0\n",
      "Iter : 23, Episode :0, Reward : 18.0\n",
      "Iter : 23, Episode :9, Reward : 45.0\n",
      "Iter : 24, Episode :0, Reward : 78.0\n",
      "Iter : 24, Episode :9, Reward : 17.0\n",
      "Iter : 25, Episode :0, Reward : 83.0\n",
      "Iter : 25, Episode :9, Reward : 103.0\n",
      "Iter : 26, Episode :0, Reward : 32.0\n",
      "Iter : 26, Episode :9, Reward : 70.0\n",
      "Iter : 27, Episode :0, Reward : 17.0\n",
      "Iter : 27, Episode :9, Reward : 14.0\n",
      "Iter : 28, Episode :0, Reward : 38.0\n",
      "Iter : 28, Episode :9, Reward : 104.0\n",
      "Iter : 29, Episode :0, Reward : 92.0\n",
      "Iter : 29, Episode :9, Reward : 110.0\n",
      "Iter : 30, Episode :0, Reward : 20.0\n",
      "Iter : 30, Episode :9, Reward : 53.0\n",
      "Iter : 31, Episode :0, Reward : 23.0\n",
      "Iter : 31, Episode :9, Reward : 75.0\n",
      "Iter : 32, Episode :0, Reward : 66.0\n",
      "Iter : 32, Episode :9, Reward : 36.0\n",
      "Iter : 33, Episode :0, Reward : 68.0\n",
      "Iter : 33, Episode :9, Reward : 27.0\n",
      "Iter : 34, Episode :0, Reward : 35.0\n",
      "Iter : 34, Episode :9, Reward : 30.0\n",
      "Iter : 35, Episode :0, Reward : 38.0\n",
      "Iter : 35, Episode :9, Reward : 94.0\n",
      "Iter : 36, Episode :0, Reward : 53.0\n",
      "Iter : 36, Episode :9, Reward : 98.0\n",
      "Iter : 37, Episode :0, Reward : 51.0\n",
      "Iter : 37, Episode :9, Reward : 17.0\n",
      "Iter : 38, Episode :0, Reward : 82.0\n",
      "Iter : 38, Episode :9, Reward : 41.0\n",
      "Iter : 39, Episode :0, Reward : 49.0\n",
      "Iter : 39, Episode :9, Reward : 26.0\n",
      "Iter : 40, Episode :0, Reward : 138.0\n",
      "Iter : 40, Episode :9, Reward : 105.0\n",
      "Iter : 41, Episode :0, Reward : 44.0\n",
      "Iter : 41, Episode :9, Reward : 29.0\n",
      "Iter : 42, Episode :0, Reward : 104.0\n",
      "Iter : 42, Episode :9, Reward : 106.0\n",
      "Iter : 43, Episode :0, Reward : 76.0\n",
      "Iter : 43, Episode :9, Reward : 67.0\n",
      "Iter : 44, Episode :0, Reward : 73.0\n",
      "Iter : 44, Episode :9, Reward : 39.0\n",
      "Iter : 45, Episode :0, Reward : 52.0\n",
      "Iter : 45, Episode :9, Reward : 82.0\n",
      "Iter : 46, Episode :0, Reward : 93.0\n",
      "Iter : 46, Episode :9, Reward : 181.0\n",
      "Iter : 47, Episode :0, Reward : 46.0\n",
      "Iter : 47, Episode :9, Reward : 89.0\n",
      "Iter : 48, Episode :0, Reward : 52.0\n",
      "Iter : 48, Episode :9, Reward : 82.0\n",
      "Iter : 49, Episode :0, Reward : 119.0\n",
      "Iter : 49, Episode :9, Reward : 113.0\n",
      "Iter : 50, Episode :0, Reward : 67.0\n",
      "Iter : 50, Episode :9, Reward : 47.0\n",
      "Iter : 51, Episode :0, Reward : 21.0\n",
      "Iter : 51, Episode :9, Reward : 46.0\n",
      "Iter : 52, Episode :0, Reward : 86.0\n",
      "Iter : 52, Episode :9, Reward : 38.0\n",
      "Iter : 53, Episode :0, Reward : 67.0\n",
      "Iter : 53, Episode :9, Reward : 62.0\n",
      "Iter : 54, Episode :0, Reward : 53.0\n",
      "Iter : 54, Episode :9, Reward : 75.0\n",
      "Iter : 55, Episode :0, Reward : 20.0\n",
      "Iter : 55, Episode :9, Reward : 173.0\n",
      "Iter : 56, Episode :0, Reward : 49.0\n",
      "Iter : 56, Episode :9, Reward : 32.0\n",
      "Iter : 57, Episode :0, Reward : 72.0\n",
      "Iter : 57, Episode :9, Reward : 35.0\n",
      "Iter : 58, Episode :0, Reward : 43.0\n",
      "Iter : 58, Episode :9, Reward : 102.0\n",
      "Iter : 59, Episode :0, Reward : 128.0\n",
      "Iter : 59, Episode :9, Reward : 111.0\n",
      "Iter : 60, Episode :0, Reward : 70.0\n",
      "Iter : 60, Episode :9, Reward : 125.0\n",
      "Iter : 61, Episode :0, Reward : 101.0\n",
      "Iter : 61, Episode :9, Reward : 119.0\n",
      "Iter : 62, Episode :0, Reward : 46.0\n",
      "Iter : 62, Episode :9, Reward : 171.0\n",
      "Iter : 63, Episode :0, Reward : 100.0\n",
      "Iter : 63, Episode :9, Reward : 99.0\n",
      "Iter : 64, Episode :0, Reward : 34.0\n",
      "Iter : 64, Episode :9, Reward : 136.0\n",
      "Iter : 65, Episode :0, Reward : 87.0\n",
      "Iter : 65, Episode :9, Reward : 179.0\n",
      "Iter : 66, Episode :0, Reward : 122.0\n",
      "Iter : 66, Episode :9, Reward : 200.0\n",
      "Iter : 67, Episode :0, Reward : 38.0\n",
      "Iter : 67, Episode :9, Reward : 76.0\n",
      "Iter : 68, Episode :0, Reward : 104.0\n",
      "Iter : 68, Episode :9, Reward : 113.0\n",
      "Iter : 69, Episode :0, Reward : 131.0\n",
      "Iter : 69, Episode :9, Reward : 192.0\n",
      "Iter : 70, Episode :0, Reward : 98.0\n",
      "Iter : 70, Episode :9, Reward : 73.0\n",
      "Iter : 71, Episode :0, Reward : 200.0\n",
      "Iter : 71, Episode :9, Reward : 200.0\n",
      "Iter : 72, Episode :0, Reward : 22.0\n",
      "Iter : 72, Episode :9, Reward : 165.0\n",
      "Iter : 73, Episode :0, Reward : 118.0\n",
      "Iter : 73, Episode :9, Reward : 43.0\n",
      "Iter : 74, Episode :0, Reward : 111.0\n",
      "Iter : 74, Episode :9, Reward : 200.0\n",
      "Iter : 75, Episode :0, Reward : 200.0\n",
      "Iter : 75, Episode :9, Reward : 154.0\n",
      "Iter : 76, Episode :0, Reward : 184.0\n",
      "Iter : 76, Episode :9, Reward : 127.0\n",
      "Iter : 77, Episode :0, Reward : 139.0\n",
      "Iter : 77, Episode :9, Reward : 166.0\n",
      "Iter : 78, Episode :0, Reward : 200.0\n",
      "Iter : 78, Episode :9, Reward : 120.0\n",
      "Iter : 79, Episode :0, Reward : 128.0\n",
      "Iter : 79, Episode :9, Reward : 86.0\n",
      "Iter : 80, Episode :0, Reward : 169.0\n",
      "Iter : 80, Episode :9, Reward : 168.0\n",
      "Iter : 81, Episode :0, Reward : 73.0\n",
      "Iter : 81, Episode :9, Reward : 133.0\n",
      "Iter : 82, Episode :0, Reward : 163.0\n",
      "Iter : 82, Episode :9, Reward : 36.0\n",
      "Iter : 83, Episode :0, Reward : 200.0\n",
      "Iter : 83, Episode :9, Reward : 68.0\n",
      "Iter : 84, Episode :0, Reward : 54.0\n",
      "Iter : 84, Episode :9, Reward : 45.0\n",
      "Iter : 85, Episode :0, Reward : 200.0\n",
      "Iter : 85, Episode :9, Reward : 72.0\n",
      "Iter : 86, Episode :0, Reward : 139.0\n",
      "Iter : 86, Episode :9, Reward : 149.0\n",
      "Iter : 87, Episode :0, Reward : 162.0\n",
      "Iter : 87, Episode :9, Reward : 200.0\n",
      "Iter : 88, Episode :0, Reward : 200.0\n",
      "Iter : 88, Episode :9, Reward : 167.0\n",
      "Iter : 89, Episode :0, Reward : 152.0\n",
      "Iter : 89, Episode :9, Reward : 148.0\n",
      "Iter : 90, Episode :0, Reward : 200.0\n",
      "Iter : 90, Episode :9, Reward : 44.0\n",
      "Iter : 91, Episode :0, Reward : 200.0\n",
      "Iter : 91, Episode :9, Reward : 131.0\n",
      "Iter : 92, Episode :0, Reward : 200.0\n",
      "Iter : 92, Episode :9, Reward : 200.0\n",
      "Iter : 93, Episode :0, Reward : 150.0\n",
      "Iter : 93, Episode :9, Reward : 39.0\n",
      "Iter : 94, Episode :0, Reward : 150.0\n",
      "Iter : 94, Episode :9, Reward : 200.0\n",
      "Iter : 95, Episode :0, Reward : 135.0\n",
      "Iter : 95, Episode :9, Reward : 200.0\n",
      "Iter : 96, Episode :0, Reward : 187.0\n",
      "Iter : 96, Episode :9, Reward : 200.0\n",
      "Iter : 97, Episode :0, Reward : 200.0\n",
      "Iter : 97, Episode :9, Reward : 200.0\n",
      "Iter : 98, Episode :0, Reward : 200.0\n",
      "Iter : 98, Episode :9, Reward : 111.0\n",
      "Iter : 99, Episode :0, Reward : 200.0\n",
      "Iter : 99, Episode :9, Reward : 120.0\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(4, activation='elu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "for iter in range(n_iter):\n",
    "    episodes_rewards = []\n",
    "    episodes_grads = []\n",
    "    for ep in range(n_ep):\n",
    "        rewards, grads = play_one_episode(n_steps, env, model, loss_fn)\n",
    "        if(ep%9 == 0):\n",
    "            print(f'Iter : {iter}, Episode :{ep}, Reward : {np.sum(rewards)}')\n",
    "        # storing rewards and grads arrays for each episode in a matrix\n",
    "        episodes_rewards.append(rewards)\n",
    "        episodes_grads.append(grads)\n",
    "\n",
    "    # Normalize and apply discounting on rewards\n",
    "    discounted_episodes_rewards = normalize_and_discount(episodes_rewards, discount_factor)\n",
    "\n",
    "    # Computing mean gradients for each model trainable vars corresponding to every step\n",
    "    var_mean_grads = []\n",
    "\n",
    "    for var_i in range(len(model.trainable_variables)):\n",
    "        mean_grad = []\n",
    "\n",
    "        for ep_i, ep in enumerate(discounted_episodes_rewards):\n",
    "            for step_i, reward in enumerate(ep):\n",
    "                mean_grad.append(reward*episodes_grads[ep_i][step_i][var_i])\n",
    "        \n",
    "        var_mean_grads.append(tf.reduce_mean(mean_grad, axis=0))\n",
    "\n",
    "    # updating model variables\n",
    "    optimizer.apply_gradients(zip(var_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "save_model(model, 'pg_with_neural_network.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
