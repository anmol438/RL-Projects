{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00880817, -0.02784138, -0.03945461,  0.02059954], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset() # [pos, vel, angle, angular vel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.009365  , -0.22237594, -0.03904263,  0.30057767], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    return int(obs[2] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward = []\n",
    "for episode in range(1000):\n",
    "    curr_reward = 0\n",
    "    obs = env.reset()[0]\n",
    "\n",
    "    for step in range(500):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, term, trunc, info = env.step(action)\n",
    "        curr_reward += reward\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    episode_reward.append(curr_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68.0, 24.0, 42.086, 8.909354858798698)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(episode_reward), np.min(episode_reward), np.mean(episode_reward), np.std(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE Algorithm with Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for number of iteration:\n",
    "#     Loop for number of episodes:\n",
    "#         Loop for number of steps:\n",
    "#             play one step and get reward and grad\n",
    "#             make an array of reward and grad\n",
    "#         store array in a matrix for each episode\n",
    "#     discount and normalize the reward matrix\n",
    "#     compute reward weighted mean for each step for every model training variables\n",
    "#     update model vars by applying gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, model, obs, loss_fn):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred_left_prob = model(obs) # binary classification => probability of going left (1 = left and 0 = right)\n",
    "        action = tf.random.uniform(shape=(1,1)) > pred_left_prob # Explore and Exploit => 0 (left) with prob of pred_left_prob and 1 (right) with prob of 1-pred_left_prob\n",
    "        y_target = [[1.]] - tf.cast(action, float) # treat it as y_true\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, pred_left_prob))\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables) # tuple => grad of loss wrt to each model var\n",
    "    obs, reward, term, trunc, info = env.step(int(action)) # perform the action\n",
    "    return obs[None,:], reward, term, trunc, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(steps, env, model, loss_fn):\n",
    "\n",
    "    obs = env.reset()[0][None,:]\n",
    "    rewards = []\n",
    "    grads = []\n",
    "    for step in range(steps):\n",
    "        obs, reward, term, trunc, grad = play_one_step(env, model, obs, loss_fn)\n",
    "        # storing reward and grad for each step in a list\n",
    "        rewards.append(reward)\n",
    "        grads.append(grad)\n",
    "\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    return rewards, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounting(rewards, discount_factor):\n",
    "    discounted_rewards = np.array(rewards)\n",
    "    for i in range(len(rewards)-2, -1, -1):\n",
    "        discounted_rewards[i] += discounted_rewards[i+1]*discount_factor\n",
    "\n",
    "    return discounted_rewards\n",
    "\n",
    "def normalize_and_discount(episodes_rewards, discount_factor):\n",
    "    \n",
    "    discounted_episodes_rewards = [discounting(rewards, discount_factor) for rewards in episodes_rewards]\n",
    "    flat_rewards = np.concatenate(discounted_episodes_rewards)\n",
    "    mean_reward = flat_rewards.mean()\n",
    "    std_reward = flat_rewards.std()\n",
    "\n",
    "    return [(discounted_rewards - mean_reward)/std_reward for discounted_rewards in discounted_episodes_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 2\n",
    "n_ep = 2\n",
    "n_steps = 10\n",
    "discount_factor = 0.95\n",
    "lr = 0.1\n",
    "\n",
    "optimizer = Adam(lr)\n",
    "loss_fn = binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(5, activation='elu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "for iter in range(n_iter):\n",
    "    episodes_rewards = []\n",
    "    episodes_grads = []\n",
    "    for ep in range(n_ep):\n",
    "        rewards, grads = play_one_episode(n_steps, env, model, loss_fn)\n",
    "        # storing rewards and grads arrays for each episode in a matrix\n",
    "        episodes_rewards.append(rewards)\n",
    "        episodes_grads.append(grads)\n",
    "\n",
    "    # Normalize and apply discounting on rewards\n",
    "    discounted_episodes_rewards = normalize_and_discount(episodes_rewards, discount_factor)\n",
    "    # print(episodes_rewards, discounted_episodes_rewards)\n",
    "    # Computing mean gradients for each model trainable vars corresponding to every step\n",
    "    var_mean_grads = []\n",
    "\n",
    "    for var_i in range(len(model.trainable_variables)):\n",
    "        mean_grad = []\n",
    "\n",
    "        for ep_i, ep in enumerate(discounted_episodes_rewards):\n",
    "            for step_i, reward in enumerate(ep):\n",
    "                mean_grad.append(reward*episodes_grads[ep_i][step_i][var_i])\n",
    "        \n",
    "        var_mean_grads.append(tf.reduce_mean(mean_grad, axis=0))\n",
    "\n",
    "    # updating model variables\n",
    "    optimizer.apply_gradients(zip(var_mean_grads, model.trainable_variables))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
